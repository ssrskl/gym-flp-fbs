digraph DQN {
    rankdir=TB;  // 绘图方向：从上到下
    node [shape=box, style="rounded,filled", fillcolor="#F0F8FF", fontname="SimHei"];

    // 算法主体
    DQN算法 [label="DQN算法", fillcolor="#E0FFFF"];
    
    // 核心组件
    DQN算法 -> {经验回放缓冲区 探索策略};
    
    // 经验回放缓冲区
    经验回放缓冲区 [label="经验回放缓冲区\n(buffer_size=1e6)\n(batch_size=32)", shape=cylinder];
    
    // 双Q网络架构
    subgraph cluster_双网络 {
        label="双Q网络架构";
        在线Q网络 [label="在线Q网络 (QNetwork)\n（MLP/CNN结构）"];
        目标Q网络 [label="目标Q网络 (QNetworkTarget)\n（延迟更新）"];
        在线Q网络 -> 目标Q网络 [label="定期同步\n(polyak_update, τ=1.0)"];
    }
    
    // 训练流程
    subgraph cluster_训练流程 {
        label="训练步骤";
        采样数据 [label="1. 采样数据\n(obs, action, reward,\n next_obs, done)"];
        计算当前Q值 [label="2. 计算当前Q值\n(使用在线网络预测)"];
        计算目标Q值 [label="3. 计算目标Q值\n(奖励 + γ * 目标网络最大Q值)"];
        计算损失 [label="4. 计算Huber损失\n(smooth_l1_loss)"];
        优化网络 [label="5. 反向传播优化\n(Adam优化器, 梯度裁剪)"];
        采样数据 -> 计算当前Q值 -> 计算损失;
        计算目标Q值 -> 计算损失;
        计算损失 -> 优化网络;
    }
    
    // 探索策略
    探索策略 [label="ε-greedy探索策略\n(初始ε=1.0 → 最终ε=0.05)\n(线性衰减比例=0.1)"];
    探索策略 -> 动作选择;
    
    // 关键超参数
    subgraph cluster_超参数 {
        label="关键超参数";
        折扣因子 [label="折扣因子 γ=0.99"];
        同步间隔 [label="目标网络更新间隔=10000步"];
        学习率 [label="学习率 lr=1e-4"];
        梯度裁剪 [label="梯度裁剪阈值=10"];
    }
    
    // 数据流向
    经验回放缓冲区 -> 采样数据 [label="随机采样"];
    在线Q网络 -> 计算当前Q值;
    目标Q网络 -> 计算目标Q值 [label="延迟更新"];
}